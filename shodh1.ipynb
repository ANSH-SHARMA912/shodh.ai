{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "An0t7veoNwXd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import roc_auc_score, f1_score, precision_recall_fscore_support\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = \"/content/drive/MyDrive/shod/accepted_2007_to_2018Q4.csv.gz\" # update to your CSV path\n",
        "PROCESSED_DIR = \"data/processed\"\n",
        "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
        "ARTIFACTS_DIR = \"artifacts\"\n",
        "os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "161BnOAoOPvq",
        "outputId": "7c1d5140-f6d3-492c-cba6-3510a39d17cb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7e4409515df0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Load data\n",
        "print(\"Loading data...\")\n",
        "df = pd.read_csv(DATA_PATH, low_memory=False)\n",
        "print(\"Loaded rows:\", len(df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHzi_0ZmOdhb",
        "outputId": "efdc0907-5d87-4362-8f0b-0a52c4cfe028"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Loaded rows: 2260701\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) EDA (light) & target mapping\n",
        "# Show basic distribution - print a few lines\n",
        "print(df.columns.tolist())\n",
        "print(df.head(3))\n",
        "\n",
        "\n",
        "# Create a consistent target: 0 = fully paid, 1 = default/charged off\n",
        "# Adjust labels according to your dataset's loan_status categories\n",
        "paid = ['Fully Paid']\n",
        "defaulted = ['Charged Off', 'Default']\n",
        "# Filter dataset to rows that belong to these categories (safe approach)\n",
        "df = df[df['loan_status'].isin(paid + defaulted)].copy()\n",
        "df['target'] = df['loan_status'].apply(lambda x: 0 if x in paid else 1)\n",
        "print(\"After filtering for clear outcomes, rows:\", len(df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWIUIrIwOoxQ",
        "outputId": "b9bcf3f1-4704-4989-9661-d74d88ab11e3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['id', 'member_id', 'loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'term', 'int_rate', 'installment', 'grade', 'sub_grade', 'emp_title', 'emp_length', 'home_ownership', 'annual_inc', 'verification_status', 'issue_d', 'loan_status', 'pymnt_plan', 'url', 'desc', 'purpose', 'title', 'zip_code', 'addr_state', 'dti', 'delinq_2yrs', 'earliest_cr_line', 'fico_range_low', 'fico_range_high', 'inq_last_6mths', 'mths_since_last_delinq', 'mths_since_last_record', 'open_acc', 'pub_rec', 'revol_bal', 'revol_util', 'total_acc', 'initial_list_status', 'out_prncp', 'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee', 'recoveries', 'collection_recovery_fee', 'last_pymnt_d', 'last_pymnt_amnt', 'next_pymnt_d', 'last_credit_pull_d', 'last_fico_range_high', 'last_fico_range_low', 'collections_12_mths_ex_med', 'mths_since_last_major_derog', 'policy_code', 'application_type', 'annual_inc_joint', 'dti_joint', 'verification_status_joint', 'acc_now_delinq', 'tot_coll_amt', 'tot_cur_bal', 'open_acc_6m', 'open_act_il', 'open_il_12m', 'open_il_24m', 'mths_since_rcnt_il', 'total_bal_il', 'il_util', 'open_rv_12m', 'open_rv_24m', 'max_bal_bc', 'all_util', 'total_rev_hi_lim', 'inq_fi', 'total_cu_tl', 'inq_last_12m', 'acc_open_past_24mths', 'avg_cur_bal', 'bc_open_to_buy', 'bc_util', 'chargeoff_within_12_mths', 'delinq_amnt', 'mo_sin_old_il_acct', 'mo_sin_old_rev_tl_op', 'mo_sin_rcnt_rev_tl_op', 'mo_sin_rcnt_tl', 'mort_acc', 'mths_since_recent_bc', 'mths_since_recent_bc_dlq', 'mths_since_recent_inq', 'mths_since_recent_revol_delinq', 'num_accts_ever_120_pd', 'num_actv_bc_tl', 'num_actv_rev_tl', 'num_bc_sats', 'num_bc_tl', 'num_il_tl', 'num_op_rev_tl', 'num_rev_accts', 'num_rev_tl_bal_gt_0', 'num_sats', 'num_tl_120dpd_2m', 'num_tl_30dpd', 'num_tl_90g_dpd_24m', 'num_tl_op_past_12m', 'pct_tl_nvr_dlq', 'percent_bc_gt_75', 'pub_rec_bankruptcies', 'tax_liens', 'tot_hi_cred_lim', 'total_bal_ex_mort', 'total_bc_limit', 'total_il_high_credit_limit', 'revol_bal_joint', 'sec_app_fico_range_low', 'sec_app_fico_range_high', 'sec_app_earliest_cr_line', 'sec_app_inq_last_6mths', 'sec_app_mort_acc', 'sec_app_open_acc', 'sec_app_revol_util', 'sec_app_open_act_il', 'sec_app_num_rev_accts', 'sec_app_chargeoff_within_12_mths', 'sec_app_collections_12_mths_ex_med', 'sec_app_mths_since_last_major_derog', 'hardship_flag', 'hardship_type', 'hardship_reason', 'hardship_status', 'deferral_term', 'hardship_amount', 'hardship_start_date', 'hardship_end_date', 'payment_plan_start_date', 'hardship_length', 'hardship_dpd', 'hardship_loan_status', 'orig_projected_additional_accrued_interest', 'hardship_payoff_balance_amount', 'hardship_last_payment_amount', 'disbursement_method', 'debt_settlement_flag', 'debt_settlement_flag_date', 'settlement_status', 'settlement_date', 'settlement_amount', 'settlement_percentage', 'settlement_term']\n",
            "         id  member_id  loan_amnt  funded_amnt  funded_amnt_inv        term  \\\n",
            "0  68407277        NaN     3600.0       3600.0           3600.0   36 months   \n",
            "1  68355089        NaN    24700.0      24700.0          24700.0   36 months   \n",
            "2  68341763        NaN    20000.0      20000.0          20000.0   60 months   \n",
            "\n",
            "   int_rate  installment grade sub_grade  ... hardship_payoff_balance_amount  \\\n",
            "0     13.99       123.03     C        C4  ...                            NaN   \n",
            "1     11.99       820.28     C        C1  ...                            NaN   \n",
            "2     10.78       432.66     B        B4  ...                            NaN   \n",
            "\n",
            "  hardship_last_payment_amount disbursement_method  debt_settlement_flag  \\\n",
            "0                          NaN                Cash                     N   \n",
            "1                          NaN                Cash                     N   \n",
            "2                          NaN                Cash                     N   \n",
            "\n",
            "  debt_settlement_flag_date settlement_status settlement_date  \\\n",
            "0                       NaN               NaN             NaN   \n",
            "1                       NaN               NaN             NaN   \n",
            "2                       NaN               NaN             NaN   \n",
            "\n",
            "  settlement_amount settlement_percentage settlement_term  \n",
            "0               NaN                   NaN             NaN  \n",
            "1               NaN                   NaN             NaN  \n",
            "2               NaN                   NaN             NaN  \n",
            "\n",
            "[3 rows x 151 columns]\n",
            "After filtering for clear outcomes, rows: 1345350\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Feature engineering & selected features\n",
        "# Convert int_rate strings like '13.56%' to numeric\n",
        "def parse_int_rate(x):\n",
        "  if pd.isna(x):\n",
        "    return np.nan\n",
        "  if isinstance(x, str) and x.endswith('%'):\n",
        "    try:\n",
        "      return float(x.strip().strip('%'))/100.0\n",
        "    except:\n",
        "      return np.nan\n",
        "  try:\n",
        "    return float(x)\n",
        "  except:\n",
        "    return np.nan\n",
        "\n",
        "\n",
        "if 'int_rate' in df.columns:\n",
        "  df['int_rate'] = df['int_rate'].apply(parse_int_rate)\n",
        "\n",
        "\n",
        "# Example selected features - adapt if your CSV lacks any of these columns\n",
        "num_features = [f for f in ['loan_amnt','int_rate','annual_inc','dti','open_acc','revol_util'] if f in df.columns]\n",
        "cat_features = [f for f in ['grade','home_ownership','purpose','emp_length','verification_status'] if f in df.columns]\n",
        "\n",
        "\n",
        "# Derived feature\n",
        "if 'annual_inc' in df.columns and 'loan_amnt' in df.columns:\n",
        "  df['income_to_loan_ratio'] = df['annual_inc'] / (df['loan_amnt'] + 1e-8)\n",
        "  derived_features = ['income_to_loan_ratio']\n",
        "else:\n",
        "  derived_features = []\n",
        "\n",
        "\n",
        "selected_features = num_features + derived_features + cat_features\n",
        "print(\"Selected features:\", selected_features)\n",
        "\n",
        "\n",
        "# Drop rows missing the selected features or target\n",
        "df_sub = df.dropna(subset=selected_features + ['target']).copy()\n",
        "print(\"After dropping NAs rows:\", len(df_sub))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F03vN3aSPQYY",
        "outputId": "0690e4c8-3de0-4cad-e2f8-dcc56e7063cb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected features: ['loan_amnt', 'int_rate', 'annual_inc', 'dti', 'open_acc', 'revol_util', 'income_to_loan_ratio', 'grade', 'home_ownership', 'purpose', 'emp_length', 'verification_status']\n",
            "After dropping NAs rows: 1266011\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Preprocessing pipeline\n",
        "# Use OneHotEncoder handle_unknown depending on sklearn version\n",
        "from sklearn import __version__ as sklearn_version\n",
        "print('scikit-learn version', sklearn_version)\n",
        "\n",
        "\n",
        "# Create pipelines\n",
        "num_pipeline = Pipeline([('scaler', StandardScaler())])\n",
        "# Use sparse_output argument for newer sklearn, else sparse\n",
        "try:\n",
        "  cat_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "except TypeError:\n",
        "  cat_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "\n",
        "\n",
        "cat_pipeline = Pipeline([('onehot', cat_encoder)])\n",
        "\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "('num', num_pipeline, num_features),\n",
        "('cat', cat_pipeline, cat_features)\n",
        "], remainder='drop')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiUUpeu2P3SR",
        "outputId": "62e70790-83c1-4074-b28a-b867f00d52d8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scikit-learn version 1.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6) Train/test split and fit preprocessor\n",
        "X = df_sub[selected_features]\n",
        "y = df_sub['target'].astype(int)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=SEED)\n",
        "\n",
        "\n",
        "print('Fitting preprocessor on train...')\n",
        "preprocessor.fit(X_train)\n",
        "X_train_proc = preprocessor.transform(X_train)\n",
        "X_test_proc = preprocessor.transform(X_test)\n",
        "\n",
        "\n",
        "# Try to get feature names for convenience\n",
        "try:\n",
        "  out_names = preprocessor.get_feature_names_out(selected_features)\n",
        "except Exception:\n",
        "  out_names = None\n",
        "\n",
        "\n",
        "# Save preprocessor\n",
        "joblib.dump(preprocessor, os.path.join(ARTIFACTS_DIR, 'preprocessor.joblib'))\n",
        "print('Saved preprocessor to artifacts')\n",
        "\n",
        "\n",
        "# Persist processed arrays\n",
        "np.save(os.path.join(PROCESSED_DIR, 'X_train_proc.npy'), X_train_proc)\n",
        "np.save(os.path.join(PROCESSED_DIR, 'X_test_proc.npy'), X_test_proc)\n",
        "\n",
        "\n",
        "y_train.to_csv(os.path.join(PROCESSED_DIR, 'y_train.csv'), index=False)\n",
        "y_test.to_csv(os.path.join(PROCESSED_DIR, 'y_test.csv'), index=False)\n",
        "\n",
        "\n",
        "print('Shapes:', X_train_proc.shape, X_test_proc.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-v7yhAHUQCdE",
        "outputId": "735d8df3-776e-4cef-854a-4041dce765d3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting preprocessor on train...\n",
            "Saved preprocessor to artifacts\n",
            "Shapes: (1012808, 47) (253203, 47)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7) Supervised classifier (PyTorch MLP)\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, input_dim):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "      nn.Linear(input_dim, 128),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout(0.2),\n",
        "      nn.Linear(128,64),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(64,1),\n",
        "      nn.Sigmoid()\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "MZVm8XYWQS7v"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to tensors\n",
        "X_train_tensor = torch.tensor(X_train_proc, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test_proc, dtype=torch.float32)\n",
        "try:\n",
        "  y_train_vals = pd.read_csv(os.path.join(PROCESSED_DIR, 'y_train.csv')).values.flatten()\n",
        "  y_test_vals = pd.read_csv(os.path.join(PROCESSED_DIR, 'y_test.csv')).values.flatten()\n",
        "except Exception:\n",
        "  y_train_vals = y_train.values\n",
        "  y_test_vals = y_test.values\n",
        "\n",
        "\n",
        "y_train_tensor = torch.tensor(y_train_vals, dtype=torch.float32).unsqueeze(1)\n",
        "y_test_tensor = torch.tensor(y_test_vals, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
        "\n",
        "\n",
        "input_dim = X_train_proc.shape[1]\n",
        "model = MLP(input_dim)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "NoL1WsWORein"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop with simple early stopping by epochs\n",
        "EPOCHS = 10\n",
        "for epoch in range(EPOCHS):\n",
        "  model.train()\n",
        "  epoch_loss = 0\n",
        "  for xb, yb in train_loader:\n",
        "    optimizer.zero_grad()\n",
        "    preds = model(xb)\n",
        "    loss = criterion(preds, yb)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "  print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {epoch_loss/len(train_loader):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05EZNds5RtOL",
        "outputId": "2aa9b690-bed7-4215-a0a8-47238235f152"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - Loss: 0.4537\n",
            "Epoch 2/10 - Loss: 0.4514\n",
            "Epoch 3/10 - Loss: 0.4509\n",
            "Epoch 4/10 - Loss: 0.4506\n",
            "Epoch 5/10 - Loss: 0.4504\n",
            "Epoch 6/10 - Loss: 0.4501\n",
            "Epoch 7/10 - Loss: 0.4500\n",
            "Epoch 8/10 - Loss: 0.4499\n",
            "Epoch 9/10 - Loss: 0.4497\n",
            "Epoch 10/10 - Loss: 0.4497\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save supervised model state_dict\n",
        "torch.save(model.state_dict(), os.path.join(ARTIFACTS_DIR, 'mlp_supervised.pth'))\n",
        "\n",
        "\n",
        "# Predict probabilities helper\n",
        "def predict_proba(model, X_numpy):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    probs = model(torch.tensor(X_numpy, dtype=torch.float32)).numpy().flatten()\n",
        "  return probs\n",
        "\n",
        "\n",
        "# Evaluate\n",
        "y_prob = predict_proba(model, X_test_proc)\n",
        "auc = roc_auc_score(y_test_vals, y_prob)\n",
        "# best threshold by maximizing F1 on test (or better: validation set)\n",
        "best_f1 = 0\n",
        "best_t = 0.5\n",
        "for t in np.linspace(0.1, 0.9, 41):\n",
        "  y_pred = (y_prob >= t).astype(int)\n",
        "  f1 = f1_score(y_test_vals, y_pred)\n",
        "  if f1 > best_f1:\n",
        "    best_f1 = f1\n",
        "    best_t = t\n",
        "\n",
        "\n",
        "print('Supervised AUC:', auc)\n",
        "print('Supervised best F1 on test:', best_f1, 'at threshold', best_t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQyLH11sR6qh",
        "outputId": "4191fffb-edc3-4185-a9db-e877224e3bbf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Supervised AUC: 0.7072591301415952\n",
            "Supervised best F1 on test: 0.4193311462566714 at threshold 0.22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8) Build RL dataset and compute rewards\n",
        "# Assumption: historical \"action\" column may not exist. We'll assume behavior_action=1 for approved rows in dataset.\n",
        "# If your dataset contains columns indicating whether an application was approved (vs. rejected), use that column.\n",
        "\n",
        "\n",
        "# For simplicity: we'll set behavior_action = 1 (approved) for all rows present (accepted loans dataset),\n",
        "# but note this biases the behavior policy. Document it in your report.\n",
        "\n",
        "\n",
        "behavior_actions = np.ones(len(df_sub), dtype=int)\n",
        "\n",
        "\n",
        "# Create rewards according to brief: if deny -> 0. if approve & fully paid -> loan_amnt * int_rate. if approve & default -> -loan_amnt\n",
        "# We need loan_amnt and int_rate in df_sub\n",
        "if 'loan_amnt' not in df_sub.columns:\n",
        "  raise ValueError('loan_amnt is required for reward calculation')\n",
        "\n",
        "\n",
        "rewards = []\n",
        "for idx, row in df_sub.iterrows():\n",
        "approved = 1 # historical\n",
        "if approved == 0:\n",
        "rewards.append(0.0)\n",
        "else:\n",
        "if row['target'] == 0:\n",
        "# fully paid -> gain interest\n",
        "ir = row['int_rate'] if 'int_rate' in row and not pd.isna(row['int_rate']) else 0.0\n",
        "rewards.append(row['loan_amnt'] * ir)\n",
        "else:\n",
        "rewards.append(-row['loan_amnt'])\n",
        "rewards = np.array(rewards, dtype=float)\n",
        "\n",
        "\n",
        "# Observations must be the preprocessed X; ensure alignment with df_sub index\n",
        "# We'll transform the whole df_sub[selected_features] using the fitted preprocessor\n",
        "X_all_proc = preprocessor.transform(df_sub[selected_features])\n",
        "\n",
        "\n",
        "# Actions: for dataset, use behavior_actions aligned to df_sub\n",
        "actions = behavior_actions # note: all ones here\n",
        "\n",
        "\n",
        "# Since d3rlpy expects episodes, we create single-step episodes (obs, action, reward, next_obs, terminal)\n",
        "# d3rlpy.MDPDataset can be built from arrays (observations, actions, rewards, terminals, next_observations)\n",
        "\n",
        "\n",
        "try:\n",
        "# next_observations are zeros or same as obs since episode ends\n",
        "terminals = np.ones(len(X_all_proc), dtype=bool)\n",
        "next_obs = np.zeros_like(X_all_proc)\n",
        "dataset = MDPDataset(observations=X_all_proc, actions=actions.reshape(-1,1), rewards=rewards.reshape(-1,1), terminals=terminals, next_observations=next_obs)\n",
        "print('Built MDPDataset for d3rlpy')\n",
        "except Exception as e:\n",
        "print('Failed to build MDPDataset:', e)\n",
        "dataset = None"
      ],
      "metadata": {
        "id": "bBjdE0s_TbF6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}